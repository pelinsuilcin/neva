import os
import json
import io
import time
from datetime import datetime
from dotenv import load_dotenv
from google.cloud import texttospeech
from google.cloud import speech
import pyaudio
import wave
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold

# --- Constants ---
MAX_HISTORY_MESSAGES = 10  # Messages to keep in immediate context
SUMMARY_INTERVAL = 6       # Messages between summaries
DATA_DIR = "data"
USER_DATA_DIR = os.path.join(DATA_DIR, "user_data")
VOICE_DATA_DIR = os.path.join(DATA_DIR, "user_voice_data")

# old system instructions for different modes
""" Senin adÄ±n Neva. Sen sÄ±cakkanlÄ±, meraklÄ±, pozitif ve esprili bir sohbet arkadaÅŸÄ±sÄ±n. 
            KullanÄ±cÄ±yla gÃ¼nlÃ¼k konular hakkÄ±nda sohbet et, ilginÃ§ bilgiler paylaÅŸ, sorular sor ve onun anÄ±larÄ±nÄ± dinle. 
            Asla psikolojik tavsiye verme. AmacÄ±n keyifli ve samimi bir diyalog kurmak. CevaplarÄ±nÄ± kÄ±sa ve doÄŸal tut.
            Ana konuÅŸma dilin TÃ¼rkÃ§e, kullanÄ±cÄ± aÃ§Ä±k ve spesifik olarak sÃ¶ylemedikÃ§e baÅŸka dillerde cevap verme. GerektikÃ§e bazÄ± inglizce temelli keliemler kullanabilirsin.
            Basit, doÄŸal ve sÄ±cak bir Ã¼slup kullan. KullanÄ±cÄ±nÄ±n geÃ§miÅŸte paylaÅŸtÄ±ÄŸÄ± kiÅŸisel detaylarÄ± hatÄ±rla.
            Ã–rnek: 'GeÃ§en konuÅŸmamÄ±zda torunun AyÅŸe'den bahsetmiÅŸtin, onun sÄ±navÄ± nasÄ±l geÃ§ti?
"""

MODES = {
    "friend": {
        "name": "ArkadaÅŸ Modu",
        "system_instruction": """
            Senin adÄ±n Neva. Sen sÄ±cakkanlÄ±, meraklÄ±, pozitif ve esprili bir sohbet arkadaÅŸÄ±sÄ±n. 
            KullanÄ±cÄ±yla gÃ¼nlÃ¼k konular hakkÄ±nda sohbet et, ilginÃ§ bilgiler paylaÅŸ, sorular sor ve onun anÄ±larÄ±nÄ± dinle. 
            Asla psikolojik tavsiye verme. AmacÄ±n keyifli ve samimi bir diyalog kurmak. CevaplarÄ±nÄ± kÄ±sa ve doÄŸal tut.
            Ana konuÅŸma dilin TÃ¼rkÃ§e,KullanÄ±cÄ± spesifik olarak istemediÄŸi sÃ¼rece tÃ¼rkÃ§e dÄ±ÅŸÄ±nda cevap verme.
            Basit, doÄŸal ve sÄ±cak bir Ã¼slup kullan.
        """
    },
    "psych": {
        "name": "Psikolojik Destek Modu",
        "system_instruction": """
            Sen bir psikolojik destek asistanÄ±sÄ±n. AmacÄ±n kullanÄ±cÄ±yÄ± dinlemek, duygularÄ±nÄ± anlamak ve onlarÄ± yargÄ±lamadan desteklemek.
            Asla teÅŸhis koyma ya da ilaÃ§ Ã¶nerme. KullanÄ±cÄ±yÄ± profesyonel bir terapiste yÃ¶nlendir.
            KullanÄ±cÄ± spesifik olarak istemediÄŸi sÃ¼rece tÃ¼rkÃ§e dÄ±ÅŸÄ±nda cevap verme. Empatik ve destekleyici bir dil kullan.
            Ã–rnek: 'Bu konuda kendinizi yalnÄ±z hissetmeniz Ã§ok doÄŸal. DuygularÄ±nÄ±zÄ± paylaÅŸtÄ±ÄŸÄ±nÄ±z iÃ§in teÅŸekkÃ¼r ederim.'
        """
    }
}

# --- Memory Management Functions ---
def get_user_filepath(user_id: str) -> str:
    """Get user-specific JSON file path"""
    os.makedirs(USER_DATA_DIR, exist_ok=True)
    return os.path.join(USER_DATA_DIR, f"neva_{user_id}.json")

def load_user_data(user_id: str) -> dict:
    filepath = get_user_filepath(user_id)
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            user_data = json.load(f)
            # Eski veri yapÄ±sÄ± kontrolÃ¼ ve dÃ¶nÃ¼ÅŸÃ¼m
            if "modes" not in user_data:
                # Eski veriyi yeni formata dÃ¶nÃ¼ÅŸtÃ¼r
                user_data = {
                    "user_id": user_data.get("user_id", user_id),
                    "created": user_data.get("created", datetime.now().isoformat()),
                    "modes": {
                        "friend": {
                            "full_history": user_data.get("full_history", []),
                            "summaries": user_data.get("summaries", []),
                            "critical_facts": user_data.get("critical_facts", []),
                            "last_summary_index": user_data.get("last_summary_index", 0)
                        },
                        "psych": {
                            "full_history": [],
                            "summaries": [],
                            "critical_facts": [],
                            "last_summary_index": 0
                        }
                    }
                }
            return user_data
    except FileNotFoundError:
        # Yeni kullanÄ±cÄ± iÃ§in varsayÄ±lan veri yapÄ±sÄ±
        return {
            "user_id": user_id,
            "created": datetime.now().isoformat(),
            "modes": {
                "friend": {
                    "full_history": [],
                    "summaries": [],
                    "critical_facts": [],
                    "last_summary_index": 0
                },
                "psych": {
                    "full_history": [],
                    "summaries": [],
                    "critical_facts": [],
                    "last_summary_index": 0
                }
            }
        }

def save_user_data(user_data: dict):
    """Save with atomic write for safety"""
    filepath = get_user_filepath(user_data["user_id"])
    temp_path = f"{filepath}.tmp"
    
    with open(temp_path, "w", encoding="utf-8") as f:
        json.dump(user_data, f, ensure_ascii=False, indent=2)
    
    os.replace(temp_path, filepath)
    print(f"Saved data for {user_data['user_id']}")

# --- Context Management ---
def build_context(user_data: dict, mode: str) -> list:
    mode_data = user_data["modes"][mode]
    context = []

    if mode_data["critical_facts"]:
        context.append({
            "role": "user",
            "parts": [f"Ã–nemli bilgiler: {', '.join(mode_data['critical_facts'])}"]
        })

    if mode_data["summaries"]:
        context.append({
            "role": "user",
            "parts": [f"Sohbet Ã¶zeti: {mode_data['summaries'][-1]}"]
        })

    # Extract just role and parts for the API (ignoring "spoken" field)
    recent_history = []
    for msg in mode_data["full_history"][-MAX_HISTORY_MESSAGES:]:
        recent_history.append({"role": msg["role"], "parts": msg["parts"]})

    context.extend(recent_history)
    return context

def needs_summarization(user_data: dict, mode: str) -> bool:
    mode_data = user_data["modes"][mode]
    new_messages = len(mode_data["full_history"]) - mode_data["last_summary_index"]
    return new_messages >= SUMMARY_INTERVAL

def generate_summary(model, user_data: dict, mode: str):
    mode_data = user_data["modes"][mode]
    history_text = "\n".join(
        f"{msg['role']}: {msg['parts'][0]}" 
        for msg in mode_data["full_history"][mode_data["last_summary_index"]:]
    )
    
    prompt = f"""
    AÅŸaÄŸÄ±daki sohbeti {'psikolojik destek' if mode == 'psych' else 'arkadaÅŸ sohbeti'} olarak, 
    aÅŸaÄŸÄ±daki odaklarla TÃœRKÃ‡E Ã¶zetle:
    1. KullanÄ±cÄ±nÄ±n duygusal durumu ve tekrarlanan endiÅŸeleri
    2. Ã–nemli kiÅŸisel detaylar
    3. Gelecek konuÅŸmalarda referans verilebilecek olaylar/anÄ±lar

    Sohbet:
    {history_text}

    Ã‡Ä±ktÄ± formatÄ±:
    Ã–zet: [en fazla 3 cÃ¼mlelik Ã¶zet]
    Ã–nemli Bilgiler: [virgÃ¼lle ayrÄ±lmÄ±ÅŸ anahtar kelimeler]
    """
    
    response = model.generate_content(prompt)
    result = response.text.strip()
    
    if "Ã–nemli Bilgiler:" in result:
        summary_part, facts_part = result.split("Ã–nemli Bilgiler:", 1)
    else:
        summary_part = result
        facts_part = ""
    
    summary_clean = summary_part.replace("Ã–zet:", "").strip()
    mode_data["summaries"].append(summary_clean)
    
    if facts_part:
        new_facts = [f.strip() for f in facts_part.split(",") if f.strip()]
        mode_data["critical_facts"].extend(new_facts)
    
    mode_data["last_summary_index"] = len(mode_data["full_history"])
    return user_data


def text_to_speech(text, output_file=None):
    # Check if output_file is None and set default with proper path
    if output_file is None:
        os.makedirs(VOICE_DATA_DIR, exist_ok=True)
        output_file = os.path.join(VOICE_DATA_DIR, "neva_speech.mp3")

    # Clean the text by removing any invalid characters
    text = text.encode('utf-8', errors='ignore').decode('utf-8')

    # Convert text to speech using Google Cloud TTS
    client = texttospeech.TextToSpeechClient()

    synthesis_input = texttospeech.SynthesisInput(text=text)

    # Configure voice - Turkish female voice
    voice = texttospeech.VoiceSelectionParams(
        language_code="tr-TR",
        ssml_gender=texttospeech.SsmlVoiceGender.FEMALE,
        name="tr-TR-Standard-A"  # You can change to a specific voice if preferred
    )

    audio_config = texttospeech.AudioConfig(
        audio_encoding=texttospeech.AudioEncoding.MP3
    )

    response = client.synthesize_speech(
        input=synthesis_input, voice=voice, audio_config=audio_config
    )

    with open(output_file, "wb") as out:
        out.write(response.audio_content)

    # Play audio based on platform
    import platform

    system = platform.system()
    if system == "Darwin":  # macOS
        os.system(f"afplay {output_file}")
    elif system == "Windows":
        os.system(f"start {output_file}")
    elif system == "Linux":
        os.system(f"mpg123 {output_file}")



def speech_to_text():
    """Record audio with improved silence detection"""
    CHUNK = 1024
    FORMAT = pyaudio.paInt16
    CHANNELS = 1
    RATE = 16000
    SILENCE_THRESHOLD = 700  # Adjust based on environment
    INITIAL_TIMEOUT = 3  # Seconds to wait for speech to begin
    SILENCE_DURATION = 1.5  # Seconds of silence to end recording
    MAX_DURATION = 15  # Maximum recording time
    os.makedirs(VOICE_DATA_DIR, exist_ok=True)
    WAVE_OUTPUT_FILENAME = os.path.join(VOICE_DATA_DIR, "user_input.wav")

    p = pyaudio.PyAudio()
    stream = p.open(format=FORMAT,
                    channels=CHANNELS,
                    rate=RATE,
                    input=True,
                    frames_per_buffer=CHUNK)

    print("Sizi dinliyorum... (konuÅŸmanÄ±z bittiÄŸinde otomatik olarak kaydedilecek)")

    frames = []
    is_speaking = False
    silence_start = None
    recording_start = time.time()

    # Track consecutive frames over threshold for better speech detection
    speech_frames_count = 0
    required_speech_frames = 3  # Require multiple frames to confirm speech

    while True:
        data = stream.read(CHUNK, exception_on_overflow=False)
        frames.append(data)

        # Calculate audio amplitude for silence detection
        amplitude = max(abs(int.from_bytes(data[i:i + 2], byteorder='little', signed=True))
                        for i in range(0, len(data), 2))

        # Visual feedback
        elapsed = time.time() - recording_start
        print(f"\rðŸŽ™ï¸ Dinliyorum {'â—' * int(elapsed)} {amplitude:4d}", end="")

        # More robust speech detection
        if amplitude > SILENCE_THRESHOLD:
            speech_frames_count += 1
            if speech_frames_count >= required_speech_frames:
                is_speaking = True
                silence_start = None
        else:
            speech_frames_count = 0
            if is_speaking:
                # Start timing silence after speech detected
                if silence_start is None:
                    silence_start = time.time()
                elif time.time() - silence_start > SILENCE_DURATION:
                    print("\nKonuÅŸma sona erdi.")
                    break
            elif time.time() - recording_start > INITIAL_TIMEOUT and not is_speaking:
                print("\nKonuÅŸma algÄ±lanmadÄ±.")
                if len(frames) <= INITIAL_TIMEOUT * RATE / CHUNK:
                    return ""  # Return empty if no speech detected initially
                break

        # Safety timeout
        if elapsed > MAX_DURATION:
            print("\nMaksimum sÃ¼re aÅŸÄ±ldÄ±.")
            break

    print("\nKaydÄ± iÅŸliyorum...")
    stream.stop_stream()
    stream.close()
    p.terminate()

    # Same processing as before
    with wave.open(WAVE_OUTPUT_FILENAME, 'wb') as wf:
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(p.get_sample_size(FORMAT))
        wf.setframerate(RATE)
        wf.writeframes(b''.join(frames))

    # Speech recognition part remains unchanged
    client = speech.SpeechClient()
    with io.open(WAVE_OUTPUT_FILENAME, "rb") as audio_file:
        content = audio_file.read()

    audio = speech.RecognitionAudio(content=content)
    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=RATE,
        language_code="tr-TR"
    )

    response = client.recognize(config=config, audio=audio)
    transcript = ""
    for result in response.results:
        transcript += result.alternatives[0].transcript

    return transcript

def is_stop_command(text):
    """Check if the input contains the specific stop command"""
    return "neva durdur" in text.lower()


# --- Main Application ---
def main():
    load_dotenv()
    genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

    # Set up Google Cloud credentials properly
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.getenv("GOOGLE_CLOUD_KEY_PATH")

    user_id = input("Ä°sminizi girin: ").strip() or "misafir"
    user_data = load_user_data(user_id)

    # Mod seÃ§imi
    print("\nLÃ¼tfen bir mod seÃ§in:")
    for key, mode in MODES.items():
        print(f"{key[0]}) {mode['name']}")

    mode_choice = input("SeÃ§iminiz (f/p): ").lower()
    selected_mode = "psych" if mode_choice == "p" else "friend"

    # Add voice interaction option
    use_voice = input("Sesli etkileÅŸim kullanmak ister misiniz? (e/h): ").lower() == 'e'

    # Modeli seÃ§ilen moda gÃ¶re oluÅŸtur
    model = genai.GenerativeModel(
        model_name='gemini-1.5-pro-latest',
        system_instruction=MODES[selected_mode]["system_instruction"],
        safety_settings={
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
        }
    )

    # Sohbeti baÅŸlat
    chat = model.start_chat(history=build_context(user_data, selected_mode))
    welcome_message = f"Merhaba {user_id}! BugÃ¼n nasÄ±lsÄ±nÄ±z?"
    print(f"\nNeva ({MODES[selected_mode]['name']}): {welcome_message}")

    # Speak welcome message if voice enabled
    if use_voice:
        text_to_speech(welcome_message)

    print("(Mod deÄŸiÅŸtirmek iÃ§in '/mod', Ã§Ä±kmak iÃ§in '/Ã§Ä±kÄ±ÅŸ' yazÄ±n)")

    current_mode = selected_mode

    while True:
        try:
            # Get user input via voice or text
            if use_voice:
                print("\nSizi dinliyorum...")
                user_input = speech_to_text()
                print(f"\nSiz: {user_input}")

                # Check for stop command in voice input
                if is_stop_command(user_input):
                    print("\nDurdurma komutu algÄ±landÄ±. Program sonlandÄ±rÄ±lÄ±yor...")
                    break
            else:
                user_input = input("\nSiz: ")

            # Ã–zel komutlarÄ± kontrol et
            if user_input.lower() == '/Ã§Ä±kÄ±ÅŸ':
                break

            elif user_input.lower() == '/mod':
                # Mod deÄŸiÅŸtirme
                print("\nLÃ¼tfen yeni mod seÃ§in:")
                for key, mode in MODES.items():
                    print(f"{key[0]}) {mode['name']}")

                mode_choice = input("SeÃ§iminiz (f/p): ").lower()
                new_mode = "psych" if mode_choice == "p" else "friend"

                if new_mode != current_mode:
                    current_mode = new_mode
                    model = genai.GenerativeModel(
                        model_name='gemini-1.5-pro-latest',
                        system_instruction=MODES[current_mode]["system_instruction"]
                    )
                    chat = model.start_chat(history=build_context(user_data, current_mode))
                    mode_change_msg = f"Mod deÄŸiÅŸtirildi! Åžimdi {MODES[current_mode]['name']} modundayÄ±m."
                    print(f"\nNeva: {mode_change_msg}")

                    if use_voice:
                        text_to_speech(mode_change_msg)
                continue

            # MesajÄ± iÅŸle
            response = chat.send_message(user_input)
            response_text = response.text

            # Dil kontrolÃ¼
            if not any(char in "Ã§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄžÄ°Ã–ÅžÃœ" for char in response_text):
                response = chat.send_message("LÃ¼tfen bu cevabÄ± TÃœRKÃ‡E olarak ver!")
                response_text = response.text

            print(f"Neva: {response_text}")

            # Convert response to speech if voice enabled
            if use_voice:
                text_to_speech(response_text)

            # GeÃ§miÅŸi gÃ¼ncelle
            mode_data = user_data["modes"][current_mode]
            mode_data["full_history"].extend([
                {"role": "user", "parts": [user_input], "spoken": use_voice},
                {"role": "model", "parts": [response_text], "spoken": use_voice}
            ])

            # Ã–zet oluÅŸtur
            if needs_summarization(user_data, current_mode):
                user_data = generate_summary(model, user_data, current_mode)
                chat = model.start_chat(history=build_context(user_data, current_mode))

        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"Hata: {str(e)}")

    # Ã‡Ä±kÄ±ÅŸta kaydet
    save_user_data(user_data)
    goodbye_message = f"GÃ¶rÃ¼ÅŸmek Ã¼zere {user_id}! Sizinle sohbet etmek gÃ¼zeldi."
    print(f"\nNeva: {goodbye_message}")

    # Speak goodbye message if voice enabled
    if use_voice:
        text_to_speech(goodbye_message)

if __name__ == "__main__":
    main()